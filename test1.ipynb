{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd123c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x233ad7c0648>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import tkinter\n",
    "#matplotlib.use('TkAgg')\n",
    "\n",
    "\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import ProjectiveTransform\n",
    "from skimage.transform import AffineTransform\n",
    "\n",
    "\n",
    "##############################################\n",
    "#\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "from models.matching import Matching\n",
    "#from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "#                          estimate_pose, make_matching_plot,\n",
    "#                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "#                          rotate_intrinsics, rotate_pose_inplane,\n",
    "#                          scale_intrinsics)\n",
    "\n",
    "#from models.utils import read_image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86632692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb6b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres\n",
    "\n",
    "cache=False\n",
    "eval=False\n",
    "fast_viz=False\n",
    "force_cpu=False\n",
    "input_dir='assets/pair/'\n",
    "input_pairs='assets/pair.txt'\n",
    "keypoint_threshold=0.005\n",
    "match_threshold=0.2\n",
    "max_keypoints=1024\n",
    "max_length=-1\n",
    "nms_radius=4\n",
    "opencv_display=False\n",
    "output_dir='resultat'\n",
    "resize=[640, 480]\n",
    "resize_float=False\n",
    "show_keypoints=False\n",
    "shuffle=False\n",
    "sinkhorn_iterations=20\n",
    "superglue='outdoor'\n",
    "viz=True\n",
    "viz_extension='png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5b819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will resize to 640x480 (WxH)\n"
     ]
    }
   ],
   "source": [
    "assert not (opencv_display and not viz), 'Must use --viz with --opencv_display'\n",
    "assert not (opencv_display and not fast_viz), 'Cannot use --opencv_display without --fast_viz'\n",
    "assert not (fast_viz and not viz), 'Must use --viz with --fast_viz'\n",
    "assert not (fast_viz and viz_extension == 'pdf'), 'Cannot use pdf extension with --fast_viz'\n",
    "\n",
    "if (len(resize) == 2) and (resize[1] == -1):    \n",
    "    resize = resize[0:1]\n",
    "if len(resize) == 2:\n",
    "    print('Will resize to {}x{} (WxH)'.format(\n",
    "        resize[0], resize[1]))\n",
    "elif len(resize) == 1 and resize[0] > 0:\n",
    "    print('Will resize max dimension to {}'.format(resize[0]))\n",
    "elif len(resize) == 1:\n",
    "    print('Will not resize images')\n",
    "else:\n",
    "    raise ValueError('Cannot specify more than two integers for --resize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1458cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11.jpg', '12.jpg']]\n",
      "Running inference on device \"cpu\"\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Looking for data in directory \"assets\\pair\"\n",
      "Will write matches to directory \"resultat\"\n",
      "Will write visualization images to directory \"resultat\"\n"
     ]
    }
   ],
   "source": [
    "    with open(input_pairs, 'r') as f:\n",
    "        pairs = [l.split() for l in f.readlines()]\n",
    "\n",
    "        print(pairs)\n",
    "\n",
    "    if max_length > -1:\n",
    "        pairs = pairs[0:np.min([len(pairs), max_length])]\n",
    "\n",
    "    if shuffle:\n",
    "        random.Random(0).shuffle(pairs)\n",
    "        \n",
    "    # Load the SuperPoint and SuperGlue models.\n",
    "    device = 'cuda' if torch.cuda.is_available() and not force_cpu else 'cpu'\n",
    "    print('Running inference on device \\\"{}\\\"'.format(device))\n",
    "    config = {\n",
    "        'superpoint': {\n",
    "            'nms_radius': nms_radius,\n",
    "            'keypoint_threshold': keypoint_threshold,\n",
    "            'max_keypoints': max_keypoints\n",
    "        },\n",
    "        'superglue': {\n",
    "            'weights': superglue,\n",
    "            'sinkhorn_iterations': sinkhorn_iterations,\n",
    "            'match_threshold': match_threshold,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    matching = Matching(config).eval().to(device)\n",
    "    \n",
    "    # Create the output directories if they do not exist already.\n",
    "    input_dir = Path(input_dir)\n",
    "    print('Looking for data in directory \\\"{}\\\"'.format(input_dir))\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print('Will write matches to directory \\\"{}\\\"'.format(output_dir))\n",
    "    if eval:\n",
    "        print('Will write evaluation results',\n",
    "              'to directory \\\"{}\\\"'.format(output_dir))\n",
    "    if viz:\n",
    "        print('Will write visualization images to',\n",
    "              'directory \\\"{}\\\"'.format(output_dir))\n",
    "    #timer = AverageTimer(newline=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560fd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteringBlackAreaKeypoints(img,keypoints):\n",
    "    new_keypoints = [[1,1]]\n",
    "    \n",
    "#     print('img.shape', img.shape)\n",
    "    \n",
    "    #print('keypoints : ',keypoints)\n",
    "\n",
    "    for i in range(len(keypoints)):\n",
    "        kpt_x = int(keypoints[i][0])\n",
    "        kpt_y = int(keypoints[i][1])\n",
    "        img_kpt = img[(kpt_y-2):(kpt_y+3),(kpt_x-2):(kpt_x+3)]\n",
    "        \n",
    "        nbr_zeros=0\n",
    "        \n",
    "        for m in range(img_kpt.shape[0]):\n",
    "            for n in range(img_kpt.shape[1]):\n",
    "                if(img_kpt[m,n]==0):\n",
    "                    nbr_zeros += 1\n",
    "                                \n",
    "#         sum_masque = np.sum(img_kpt)\n",
    "#         print('sum : ',sum_masque)\n",
    "#         if(sum_masque >  2000):\n",
    "        #print('nbr_zeros :',nbr_zeros)\n",
    "        if (nbr_zeros<2):\n",
    "#             plt.figure(figsize=(10, 10))\n",
    "#             plt.imshow(img_kpt)\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "#             print(img_kpt)\n",
    "            new_keypoints = np.concatenate((new_keypoints,[keypoints[i]]),axis=0)\n",
    "        \n",
    "    #print('new_keypoints[1:] : ',new_keypoints[1:])\n",
    "            \n",
    "    return new_keypoints[1:]\n",
    "\n",
    "def process_resize(w, h, resize):\n",
    "    assert(len(resize) > 0 and len(resize) <= 2)\n",
    "    if len(resize) == 1 and resize[0] > -1:\n",
    "        scale = resize[0] / max(h, w)\n",
    "        w_new, h_new = int(round(w*scale)), int(round(h*scale))\n",
    "    elif len(resize) == 1 and resize[0] == -1:\n",
    "        w_new, h_new = w, h\n",
    "    else:  # len(resize) == 2:\n",
    "        w_new, h_new = resize[0], resize[1]\n",
    "\n",
    "    # Issue warning if resolution is too small or too large.\n",
    "    if max(w_new, h_new) < 160:\n",
    "        print('Warning: input resolution is very small, results may vary')\n",
    "    elif max(w_new, h_new) > 2000:\n",
    "        print('Warning: input resolution is very large, results may vary')\n",
    "\n",
    "    return w_new, h_new\n",
    "\n",
    "def frame2tensor(frame, device):\n",
    "    return torch.from_numpy(frame/255.).float()[None, None].to(device)\n",
    "\n",
    "def read_image(path, device, resize, rotation, resize_float):\n",
    "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        return None, None, None\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    w_new, h_new = process_resize(w, h, resize)\n",
    "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
    "\n",
    "    if resize_float:\n",
    "        image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
    "    else:\n",
    "        image = cv2.resize(image, (w_new, h_new)).astype('float32')\n",
    "\n",
    "    if rotation != 0:\n",
    "        image = np.rot90(image, k=rotation)\n",
    "        if rotation % 2:\n",
    "            scales = scales[::-1]\n",
    "\n",
    "    inp = frame2tensor(image, device)\n",
    "    return image, inp, scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2651fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n",
      "hola1\n",
      "hola2\n",
      "hola22\n"
     ]
    }
   ],
   "source": [
    "for i, pair in enumerate(pairs):\n",
    "    print('hola')\n",
    "    name0, name1 = pair[:2]\n",
    "    stem0, stem1 = Path(name0).stem, Path(name1).stem\n",
    "    matches_path = output_dir / '{}_{}_matches.npz'.format(stem0, stem1)\n",
    "    eval_path = output_dir / '{}_{}_evaluation.npz'.format(stem0, stem1)\n",
    "    viz_path = output_dir / '{}_{}_matches.{}'.format(stem0, stem1, viz_extension)\n",
    "    viz_eval_path = output_dir / \\\n",
    "        '{}_{}_evaluation.{}'.format(stem0, stem1, viz_extension)\n",
    "\n",
    "    # Handle --cache logic.\n",
    "    do_match = True\n",
    "    do_eval = eval\n",
    "    do_viz = viz\n",
    "    do_viz_eval = eval and viz\n",
    "\n",
    "    if not (do_match or do_eval or do_viz or do_viz_eval):\n",
    "        timer.print('Finished pair {:5} of {:5}'.format(i, len(pairs)))\n",
    "        continue\n",
    "\n",
    "    # If a rotation integer is provided (e.g. from EXIF data), use it:\n",
    "    if len(pair) >= 5:\n",
    "        rot0, rot1 = int(pair[2]), int(pair[3])\n",
    "    else:\n",
    "        rot0, rot1 = 0, 0\n",
    "    print('hola1')\n",
    "    # Load the image pair.\n",
    "    image0, inp0, scales0 = read_image(\n",
    "        input_dir / name0, device, resize, rot0, resize_float)\n",
    "    image1, inp1, scales1 = read_image(\n",
    "        input_dir / name1, device, resize, rot1, resize_float)\n",
    "    if image0 is None or image1 is None:\n",
    "        print('Problem reading image pair: {} {}'.format(\n",
    "            input_dir/name0, input_dir/name1))\n",
    "        exit(1)\n",
    "    #timer.update('load_image')\n",
    "    \n",
    "    print('hola2')\n",
    "\n",
    "    if do_match:\n",
    "        # Perform the matching.\n",
    "        print('hola22')\n",
    "        pred = matching({'image0': inp0, 'image1': inp1})\n",
    "        print('hola24')\n",
    "        pred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n",
    "        print('hola25')\n",
    "        kpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n",
    "        matches, conf = pred['matches0'], pred['matching_scores0']\n",
    "        #timer.update('matcher')\n",
    "        \n",
    "        print('hola3')\n",
    "\n",
    "        # Write the matches to disk.\n",
    "        out_matches = {'keypoints0': kpts0, 'keypoints1': kpts1,\n",
    "                       'matches': matches, 'match_confidence': conf}\n",
    "        np.savez(str(matches_path), **out_matches)\n",
    "\n",
    "    print('hola4')\n",
    "\n",
    "    # Keep the matching keypoints.\n",
    "    valid = matches > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[matches[valid]]\n",
    "    mconf = conf[valid]\n",
    "\n",
    "    print(mkpts0.shape)\n",
    "    print(mkpts1.shape)\n",
    "    '''\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image0,cmap='gray')\n",
    "    plt.plot(mkpts0[:, 0], mkpts0[:, 1], \"og\", markersize=5)  # og:shorthand for green circle\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    '''\n",
    "    mkpts0 = filteringBlackAreaKeypoints(image0,mkpts0)\n",
    "    '''\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image0,cmap='gray')\n",
    "    plt.plot(mkpts0[:, 0], mkpts0[:, 1], \"og\", markersize=5)  # og:shorthand for green circle\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    model1, inliers = ransac(\n",
    "        (mkpts0, mkpts1),\n",
    "        ProjectiveTransform, min_samples=4,\n",
    "        residual_threshold=4, max_trials=10000\n",
    "    )\n",
    "    n_inliers = np.sum(inliers)\n",
    "    print('Number of inliers: %d.' % n_inliers)\n",
    "    inlier_keypoints_left = [cv2.KeyPoint(point[0], point[1], 1) for point in mkpts0[inliers]]\n",
    "    inlier_keypoints_right = [cv2.KeyPoint(point[0], point[1], 1) for point in mkpts1[inliers]]\n",
    "    placeholder_matches = [cv2.DMatch(idx, idx, 1) for idx in range(n_inliers)]\n",
    "    image_r1 = cv2.drawMatches(image0, inlier_keypoints_left, image1, inlier_keypoints_right, placeholder_matches, None)\n",
    "    \n",
    "    print(model1.params)\n",
    "    T_x = model1.params[0,2]*model1.params[0,0]+model1.params[1,2]*model1.params[1,0]\n",
    "    T_y = model1.params[1,2]*model1.params[0,0]-model1.params[0,2]*model1.params[1,0]\n",
    "\n",
    "    print('Transltation : (',model1.params[0,2],',', model1.params[1,2],')')\n",
    "    print('Transltation2 : (',T_x,',', T_y,')')\n",
    "    \n",
    "    \n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    #plt.imshow(image_r1)\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "'''\n",
    "        if do_viz:\n",
    "            # Visualize the matches.\n",
    "            color = cm.jet(mconf)\n",
    "            text = [\n",
    "                'SuperGlue',\n",
    "                'Keypoints: {}:{}'.format(len(kpts0), len(kpts1)),\n",
    "                'Matches: {}'.format(len(mkpts0)),\n",
    "            ]\n",
    "            if rot0 != 0 or rot1 != 0:\n",
    "                text.append('Rotation: {}:{}'.format(rot0, rot1))\n",
    "\n",
    "            # Display extra parameter info.\n",
    "            k_thresh = matching.superpoint.config['keypoint_threshold']\n",
    "            m_thresh = matching.superglue.config['match_threshold']\n",
    "            small_text = [\n",
    "                'Keypoint Threshold: {:.4f}'.format(k_thresh),\n",
    "                'Match Threshold: {:.2f}'.format(m_thresh),\n",
    "                'Image Pair: {}:{}'.format(stem0, stem1),\n",
    "            ]\n",
    "\n",
    "            make_matching_plot(\n",
    "                image0, image1, kpts0, kpts1, mkpts0, mkpts1, color,\n",
    "                text, viz_path, show_keypoints,\n",
    "                fast_viz, opencv_display, 'Matches', small_text)\n",
    "\n",
    "            timer.update('viz_match')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1019b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image0)\n",
    "plt.plot(kpts0[:, 0], kpts0[:, 1], \"og\", markersize=5)  # og:shorthand for green circle\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8704c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f2b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a903c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e1a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8725507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabef0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee44f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
